


# ğŸ“ Automated Answer Sheet Grading System

**AI-powered evaluation of handwritten student answers with feedback, accuracy, and elegance.**

![App Demo](screenshot/preview.gif)

Welcome to the **Automated Answer Sheet Grading System** â€” an advanced platform built using cutting-edge **AI, NLP, OCR, and LLMs** that automates the evaluation of handwritten student responses from scanned answer sheets. Designed to provide accurate scores **and personalized feedback**, this project makes grading efficient, insightful, and scalable.

---

## ğŸ“Œ Table of Contents

* [ğŸš€ Features](#-features)
* [ğŸ§  Tech Stack](#-tech-stack)
* [ğŸ–¼ï¸ Architecture](#-architecture)
* [âš™ï¸ Installation](#-installation)
* [ğŸŒ Usage](#-usage)
* [ğŸ” Evaluation Pipeline](#-evaluation-pipeline)
* [ğŸ“‚ File Structure](#-file-structure)
* [ğŸ¤ Contributing](#-contributing)
* [ğŸ“œ License](#-license)

---

## ğŸš€ Features

âœ… **Automatic Grading** of handwritten answers from scanned PDFs
âœ… **Google Vision OCR API** for accurate handwritten text extraction
âœ… **Mistral 7B (via Ollama)** for deep semantic evaluation
âœ… **Advanced NLP** using techniques like cosine similarity, sentiment analysis, coherence scoring
âœ… **Feedback Generator** for each answer
âœ… **Interactive UI** built with Flask & Bootstrap
âœ… **High Accuracy (89%)** across various answer formats
âœ… **Works on PDF Answer Sheets** with clean output and scoring breakdown

---

## ğŸ§  Tech Stack

| Technology                            | Role                                |
| ------------------------------------- | ----------------------------------- |
| **Python**                            | Core programming language           |
| **Flask**                             | Web framework                       |
| **Google Vision API**                 | OCR for extracting handwritten text |
| **Mistral 7B (Ollama)**               | LLM for deep answer evaluation      |
| **NLP (NLTK, Sklearn, Transformers)** | Text processing & similarity        |
| **Bootstrap, HTML/CSS**               | Frontend design                     |
| **Jupyter Notebooks**                 | Development and testing             |

---

## ğŸ–¼ï¸ Architecture

```mermaid
graph TD
A[PDF Upload] --> B[Google Vision API - OCR]
B --> C[Extracted Answer Texts]
C --> D[Mistral 7B - Answer Evaluation]
D --> E[NLP & ML Scoring Pipeline]
E --> F[Feedback Generator]
F --> G[Final Score + Feedback]
G --> H[Frontend Display]
```

---

## âš™ï¸ Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/yourusername/answer-evaluation-system.git
   cd answer-evaluation-system
   ```

2. **Create a virtual environment**

   ```bash
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

3. **Install required packages**

   ```bash
   pip install -r requirements.txt
   ```

4. **Start Ollama & Load Mistral 7B**

   ```bash
   ollama run mistral
   ```

5. **Run the application**

   ```bash
   python admin.py
   ```

---

## ğŸŒ Usage

1. Open your browser and go to `http://127.0.0.1:5000`
2. Upload a scanned PDF answer sheet
3. Click "Evaluate"
4. View:

   * Individual question scores
   * Personalized feedback
   * Overall score

---

## ğŸ” Evaluation Pipeline

Each answer is passed through a pipeline of models and scoring functions:

* ğŸ”¤ **OCR**: Handwriting extracted using Google Vision API
* ğŸ§½ **Preprocessing**: Tokenization, Lemmatization
* ğŸ“Š **Matching Metrics**:

  * Exact Match
  * Partial Token Overlap
  * Cosine Similarity (TF-IDF)
* â¤ï¸ **Sentiment Analysis**: Understands tone and effort
* ğŸ’¡ **Semantic Scoring**: Using Mistral 7B (via Ollama)
* ğŸ§  **ML Classification**: Naive Bayes for probabilistic scoring
* ğŸ§­ **Feedback**: Dynamic feedback based on gaps/match
* âš–ï¸ **Weighted Score**: Final score based on multiple criteria

---

## ğŸ“‚ File Structure

```bash
answer-evaluation-system/
â”œâ”€â”€ admin.py                  # Main Flask app
â”œâ”€â”€ ocr_module.py             # Google Vision OCR logic
â”œâ”€â”€ evaluation.py             # Scoring logic and ML models
â”œâ”€â”€ feedback_generator.py     # Personalized feedback generation
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html            # Frontend UI
â”œâ”€â”€ static/
â”‚   â””â”€â”€ styles.css            # UI styling
â”œâ”€â”€ screenshot/
â”‚   â”œâ”€â”€ preview.gif           # App preview
â”‚   â””â”€â”€ evaluation_demo.gif   # Evaluation process
â””â”€â”€ requirements.txt          # Dependencies
```

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:

* â­ Star this repo
* ğŸ› Report issues
* ğŸ“¥ Submit PRs with improvements or new features

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ“£ Acknowledgements

* [Google Cloud Vision API](https://cloud.google.com/vision)
* [Ollama + Mistral 7B](https://ollama.com)
* [HuggingFace Transformers](https://huggingface.co)
* \[NLTK, Scikit-learn, Flask]

---

## ğŸ” Authentication

To use the Google Vision API, you need to provide a **service account key** from Google Cloud.

1. Go to your [Google Cloud Console](https://console.cloud.google.com/).
2. Create a service account and download the credentials JSON.
3. Rename it to `service_account.json` and place it in the project root.
4. **Do NOT commit this file.** It's listed in `.gitignore` for your safety.

You can refer to `service_account_example.json` for the required structure.

